{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1333,"status":"ok","timestamp":1673600509982,"user":{"displayName":"Raj Kapadia","userId":"02452332357525858055"},"user_tz":-330},"id":"1CDfu7ZJJn93","outputId":"27c903d1-a6c5-4d31-bc01-4180180a6608"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'stylegan2-ada-pytorch'...\n","remote: Enumerating objects: 128, done.\u001b[K\n","remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n","Receiving objects: 100% (128/128), 1.12 MiB | 6.01 MiB/s, done.\n","Resolving deltas: 100% (57/57), done.\n"]}],"source":["!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":459,"status":"ok","timestamp":1673600514406,"user":{"displayName":"Raj Kapadia","userId":"02452332357525858055"},"user_tz":-330},"id":"ldPUX0WKJ0C-","outputId":"899f3598-4f3d-456d-aa5f-910290131e97"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/stylegan2-ada-pytorch\n"]}],"source":["%cd /content/stylegan2-ada-pytorch"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1673600517746,"user":{"displayName":"Raj Kapadia","userId":"02452332357525858055"},"user_tz":-330},"id":"ckW2qf25J4TO","outputId":"7c789cca-6ca1-4878-d22a-a64f02b2f90d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[01;32mcalc_metrics.py\u001b[0m*  \u001b[01;32mdocker_run.sh\u001b[0m*  \u001b[01;32mLICENSE.txt\u001b[0m*   \u001b[01;32mstyle_mixing.py\u001b[0m*\n","\u001b[01;32mdataset_tool.py\u001b[0m*  \u001b[01;34mdocs\u001b[0m/           \u001b[01;34mmetrics\u001b[0m/       \u001b[01;34mtorch_utils\u001b[0m/\n","\u001b[01;34mdnnlib\u001b[0m/           \u001b[01;32mgenerate.py\u001b[0m*    \u001b[01;32mprojector.py\u001b[0m*  \u001b[01;34mtraining\u001b[0m/\n","\u001b[01;32mDockerfile\u001b[0m*       \u001b[01;32mlegacy.py\u001b[0m*      \u001b[01;32mREADME.md\u001b[0m*     \u001b[01;32mtrain.py\u001b[0m*\n"]}],"source":["%ls"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":91687,"status":"ok","timestamp":1673600630681,"user":{"displayName":"Raj Kapadia","userId":"02452332357525858055"},"user_tz":-330},"id":"haH-YOyrJ70X","outputId":"c32494db-db19-45f0-e3a0-fe8a3568e2e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.7.1+cu110\n","  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp38-cp38-linux_x86_64.whl (1156.8 MB)\n","\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 GB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1156767744 bytes == 0x3056000 @  0x7f844ce591e7 0x4d30a0 0x4d312c 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4997a2\n","\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 GB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1445961728 bytes == 0x47f84000 @  0x7f844ce5a615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 GB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision==0.8.2+cu110\n","  Downloading https://download.pytorch.org/whl/cu110/torchvision-0.8.2%2Bcu110-cp38-cp38-linux_x86_64.whl (12.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.7.1+cu110) (4.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.7.1+cu110) (1.21.6)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.8.2+cu110) (7.1.2)\n","Installing collected packages: torch, torchvision\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.0+cu116\n","    Uninstalling torch-1.13.0+cu116:\n","      Successfully uninstalled torch-1.13.0+cu116\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.14.0+cu116\n","    Uninstalling torchvision-0.14.0+cu116:\n","      Successfully uninstalled torchvision-0.14.0+cu116\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.7.1+cu110 which is incompatible.\n","torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.7.1+cu110 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.7.1+cu110 torchvision-0.8.2+cu110\n"]}],"source":["%pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6661,"status":"ok","timestamp":1673600667538,"user":{"displayName":"Raj Kapadia","userId":"02452332357525858055"},"user_tz":-330},"id":"5TJqU_2UKdef","outputId":"ee487bbb-32e3-46e1-c452-52f6ddbaaab5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (2.25.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n","Collecting pyspng\n","  Downloading pyspng-0.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.9/205.9 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ninja\n","  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting imageio-ffmpeg==0.4.3\n","  Downloading imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests) (2022.12.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pyspng) (1.21.6)\n","Installing collected packages: ninja, pyspng, imageio-ffmpeg\n","Successfully installed imageio-ffmpeg-0.4.3 ninja-1.11.1 pyspng-0.1.1\n"]}],"source":["%pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":534,"status":"ok","timestamp":1673600702169,"user":{"displayName":"Raj Kapadia","userId":"02452332357525858055"},"user_tz":-330},"id":"OEGkJzGAX9iV","outputId":"314a3567-142c-4bbf-ea41-acc678300db0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Usage: dataset_tool.py [OPTIONS]\n","\n","  Convert an image dataset into a dataset\n","  archive usable with StyleGAN2 ADA PyTorch.\n","\n","  The input dataset format is guessed from the\n","  --source argument:\n","\n","  --source *_lmdb/                    Load LSUN dataset\n","  --source cifar-10-python.tar.gz     Load CIFAR-10 dataset\n","  --source train-images-idx3-ubyte.gz Load MNIST dataset\n","  --source path/                      Recursively load all images from path/\n","  --source dataset.zip                Recursively load all images from dataset.zip\n","\n","  Specifying the output format and path:\n","\n","  --dest /path/to/dir                 Save output files under /path/to/dir\n","  --dest /path/to/dataset.zip         Save output files into /path/to/dataset.zip\n","\n","  The output dataset format can be either an\n","  image folder or an uncompressed zip archive.\n","  Zip archives makes it easier to move datasets\n","  around file servers and clusters, and may\n","  offer better training performance on network\n","  file systems.\n","\n","  Images within the dataset archive will be\n","  stored as uncompressed PNG. Uncompresed PNGs\n","  can be efficiently decoded in the training\n","  loop.\n","\n","  Class labels are stored in a file called\n","  'dataset.json' that is stored at the dataset\n","  root folder.  This file has the following\n","  structure:\n","\n","  {\n","      \"labels\": [\n","          [\"00000/img00000000.png\",6],\n","          [\"00000/img00000001.png\",9],\n","          ... repeated for every image in the datase\n","          [\"00049/img00049999.png\",1]\n","      ]\n","  }\n","\n","  If the 'dataset.json' file cannot be found,\n","  the dataset is interpreted as not containing\n","  class labels.\n","\n","  Image scale/crop and resolution requirements:\n","\n","  Output images must be square-shaped and they\n","  must all have the same power-of-two\n","  dimensions.\n","\n","  To scale arbitrary input image size to a\n","  specific width and height, use the --width and\n","  --height options.  Output resolution will be\n","  either the original input resolution (if\n","  --width/--height was not specified) or the one\n","  specified with --width/height.\n","\n","  Use the --transform=center-crop or\n","  --transform=center-crop-wide options to apply\n","  a center crop transform on the input image.\n","  These options should be used with the --width\n","  and --height options.  For example:\n","\n","  python dataset_tool.py --source LSUN/raw/cat_lmdb --dest /tmp/lsun_cat \\\n","      --transform=center-crop-wide --width 512 --height=384\n","\n","Options:\n","  --source PATH                   Directory or\n","                                  archive name for\n","                                  input dataset\n","                                  [required]\n","\n","  --dest PATH                     Output directory\n","                                  or archive name\n","                                  for output\n","                                  dataset\n","                                  [required]\n","\n","  --max-images INTEGER            Output only up\n","                                  to `max-images`\n","                                  images\n","\n","  --resize-filter [box|lanczos]   Filter to use\n","                                  when resizing\n","                                  images for\n","                                  output\n","                                  resolution\n","                                  [default:\n","                                  lanczos]\n","\n","  --transform [center-crop|center-crop-wide]\n","                                  Input\n","                                  crop/resize mode\n","\n","  --width INTEGER                 Output width\n","  --height INTEGER                Output height\n","  --help                          Show this\n","                                  message and\n","                                  exit.\n"]}],"source":["!python dataset_tool.py --help"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6093,"status":"ok","timestamp":1673600733388,"user":{"displayName":"Raj Kapadia","userId":"02452332357525858055"},"user_tz":-330},"id":"d-ZkuLOu9a-d","outputId":"f5996fe4-9cb9-4ce5-ca5a-69541f7f21ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["100% 10/10 [00:05<00:00,  1.88it/s]\n"]}],"source":["!python dataset_tool.py --source=/content/drive/MyDrive/StyleGAN/biked --dest=/content/dataset.zip --width=128 --height=128"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2373,"status":"ok","timestamp":1673600756297,"user":{"displayName":"Raj Kapadia","userId":"02452332357525858055"},"user_tz":-330},"id":"nX2TF5W9Lhk-","outputId":"1a0fcf2c-6021-4c2d-dc91-eb20f48ae837"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training options:\n","{\n","  \"num_gpus\": 1,\n","  \"image_snapshot_ticks\": 50,\n","  \"network_snapshot_ticks\": 50,\n","  \"metrics\": [\n","    \"fid50k_full\"\n","  ],\n","  \"random_seed\": 0,\n","  \"training_set_kwargs\": {\n","    \"class_name\": \"training.dataset.ImageFolderDataset\",\n","    \"path\": \"/content/dataset.zip\",\n","    \"use_labels\": false,\n","    \"max_size\": 10,\n","    \"xflip\": false,\n","    \"resolution\": 128\n","  },\n","  \"data_loader_kwargs\": {\n","    \"pin_memory\": true,\n","    \"num_workers\": 3,\n","    \"prefetch_factor\": 2\n","  },\n","  \"G_kwargs\": {\n","    \"class_name\": \"training.networks.Generator\",\n","    \"z_dim\": 512,\n","    \"w_dim\": 512,\n","    \"mapping_kwargs\": {\n","      \"num_layers\": 2\n","    },\n","    \"synthesis_kwargs\": {\n","      \"channel_base\": 16384,\n","      \"channel_max\": 512,\n","      \"num_fp16_res\": 4,\n","      \"conv_clamp\": 256\n","    }\n","  },\n","  \"D_kwargs\": {\n","    \"class_name\": \"training.networks.Discriminator\",\n","    \"block_kwargs\": {},\n","    \"mapping_kwargs\": {},\n","    \"epilogue_kwargs\": {\n","      \"mbstd_group_size\": 4\n","    },\n","    \"channel_base\": 16384,\n","    \"channel_max\": 512,\n","    \"num_fp16_res\": 4,\n","    \"conv_clamp\": 256\n","  },\n","  \"G_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.0025,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"D_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.0025,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"loss_kwargs\": {\n","    \"class_name\": \"training.loss.StyleGAN2Loss\",\n","    \"r1_gamma\": 0.1024\n","  },\n","  \"total_kimg\": 25000,\n","  \"batch_size\": 32,\n","  \"batch_gpu\": 32,\n","  \"ema_kimg\": 10.0,\n","  \"ema_rampup\": 0.05,\n","  \"ada_target\": 0.6,\n","  \"augment_kwargs\": {\n","    \"class_name\": \"training.augment.AugmentPipe\",\n","    \"xflip\": 1,\n","    \"rotate90\": 1,\n","    \"xint\": 1,\n","    \"scale\": 1,\n","    \"rotate\": 1,\n","    \"aniso\": 1,\n","    \"xfrac\": 1,\n","    \"brightness\": 1,\n","    \"contrast\": 1,\n","    \"lumaflip\": 1,\n","    \"hue\": 1,\n","    \"saturation\": 1\n","  },\n","  \"run_dir\": \"/content/training-runs/00000-dataset-auto1\"\n","}\n","\n","Output directory:   /content/training-runs/00000-dataset-auto1\n","Training data:      /content/dataset.zip\n","Training duration:  25000 kimg\n","Number of GPUs:     1\n","Number of images:   10\n","Image resolution:   128\n","Conditional model:  False\n","Dataset x-flips:    False\n","\n","Dry run; exiting.\n"]}],"source":["!python train.py --outdir=/content/training-runs --data=/content/dataset.zip --dry-run"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1601718,"status":"ok","timestamp":1673602693002,"user":{"displayName":"Raj Kapadia","userId":"02452332357525858055"},"user_tz":-330},"id":"UOWvLhHBTZ1A","outputId":"f16fb080-faf9-4507-83f2-fef22b7f6b47"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training options:\n","{\n","  \"num_gpus\": 1,\n","  \"image_snapshot_ticks\": 50,\n","  \"network_snapshot_ticks\": 50,\n","  \"metrics\": [\n","    \"fid50k_full\"\n","  ],\n","  \"random_seed\": 0,\n","  \"training_set_kwargs\": {\n","    \"class_name\": \"training.dataset.ImageFolderDataset\",\n","    \"path\": \"/content/dataset.zip\",\n","    \"use_labels\": false,\n","    \"max_size\": 10,\n","    \"xflip\": false,\n","    \"resolution\": 128\n","  },\n","  \"data_loader_kwargs\": {\n","    \"pin_memory\": true,\n","    \"num_workers\": 3,\n","    \"prefetch_factor\": 2\n","  },\n","  \"G_kwargs\": {\n","    \"class_name\": \"training.networks.Generator\",\n","    \"z_dim\": 512,\n","    \"w_dim\": 512,\n","    \"mapping_kwargs\": {\n","      \"num_layers\": 2\n","    },\n","    \"synthesis_kwargs\": {\n","      \"channel_base\": 16384,\n","      \"channel_max\": 512,\n","      \"num_fp16_res\": 4,\n","      \"conv_clamp\": 256\n","    }\n","  },\n","  \"D_kwargs\": {\n","    \"class_name\": \"training.networks.Discriminator\",\n","    \"block_kwargs\": {},\n","    \"mapping_kwargs\": {},\n","    \"epilogue_kwargs\": {\n","      \"mbstd_group_size\": 4\n","    },\n","    \"channel_base\": 16384,\n","    \"channel_max\": 512,\n","    \"num_fp16_res\": 4,\n","    \"conv_clamp\": 256\n","  },\n","  \"G_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.0025,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"D_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.0025,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"loss_kwargs\": {\n","    \"class_name\": \"training.loss.StyleGAN2Loss\",\n","    \"r1_gamma\": 0.1024\n","  },\n","  \"total_kimg\": 10,\n","  \"batch_size\": 32,\n","  \"batch_gpu\": 32,\n","  \"ema_kimg\": 10.0,\n","  \"ema_rampup\": 0.05,\n","  \"ada_target\": 0.6,\n","  \"augment_kwargs\": {\n","    \"class_name\": \"training.augment.AugmentPipe\",\n","    \"xflip\": 1,\n","    \"rotate90\": 1,\n","    \"xint\": 1,\n","    \"scale\": 1,\n","    \"rotate\": 1,\n","    \"aniso\": 1,\n","    \"xfrac\": 1,\n","    \"brightness\": 1,\n","    \"contrast\": 1,\n","    \"lumaflip\": 1,\n","    \"hue\": 1,\n","    \"saturation\": 1\n","  },\n","  \"run_dir\": \"/content/training-runs/00001-dataset-auto1-kimg10\"\n","}\n","\n","Output directory:   /content/training-runs/00001-dataset-auto1-kimg10\n","Training data:      /content/dataset.zip\n","Training duration:  10 kimg\n","Number of GPUs:     1\n","Number of images:   10\n","Image resolution:   128\n","Conditional model:  False\n","Dataset x-flips:    False\n","\n","Creating output directory...\n","Launching processes...\n","Loading training set...\n","\n","Num images:  10\n","Image shape: [3, 128, 128]\n","Label shape: [0]\n","\n","Constructing networks...\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n","\n","Generator             Parameters  Buffers  Output shape         Datatype\n","---                   ---         ---      ---                  ---     \n","mapping.fc0           262656      -        [32, 512]            float32 \n","mapping.fc1           262656      -        [32, 512]            float32 \n","mapping               -           512      [32, 12, 512]        float32 \n","synthesis.b4.conv1    2622465     32       [32, 512, 4, 4]      float32 \n","synthesis.b4.torgb    264195      -        [32, 3, 4, 4]        float32 \n","synthesis.b4:0        8192        16       [32, 512, 4, 4]      float32 \n","synthesis.b4:1        -           -        [32, 512, 4, 4]      float32 \n","synthesis.b8.conv0    2622465     80       [32, 512, 8, 8]      float32 \n","synthesis.b8.conv1    2622465     80       [32, 512, 8, 8]      float32 \n","synthesis.b8.torgb    264195      -        [32, 3, 8, 8]        float32 \n","synthesis.b8:0        -           16       [32, 512, 8, 8]      float32 \n","synthesis.b8:1        -           -        [32, 512, 8, 8]      float32 \n","synthesis.b16.conv0   2622465     272      [32, 512, 16, 16]    float16 \n","synthesis.b16.conv1   2622465     272      [32, 512, 16, 16]    float16 \n","synthesis.b16.torgb   264195      -        [32, 3, 16, 16]      float16 \n","synthesis.b16:0       -           16       [32, 512, 16, 16]    float16 \n","synthesis.b16:1       -           -        [32, 512, 16, 16]    float32 \n","synthesis.b32.conv0   2622465     1040     [32, 512, 32, 32]    float16 \n","synthesis.b32.conv1   2622465     1040     [32, 512, 32, 32]    float16 \n","synthesis.b32.torgb   264195      -        [32, 3, 32, 32]      float16 \n","synthesis.b32:0       -           16       [32, 512, 32, 32]    float16 \n","synthesis.b32:1       -           -        [32, 512, 32, 32]    float32 \n","synthesis.b64.conv0   1442561     4112     [32, 256, 64, 64]    float16 \n","synthesis.b64.conv1   721409      4112     [32, 256, 64, 64]    float16 \n","synthesis.b64.torgb   132099      -        [32, 3, 64, 64]      float16 \n","synthesis.b64:0       -           16       [32, 256, 64, 64]    float16 \n","synthesis.b64:1       -           -        [32, 256, 64, 64]    float32 \n","synthesis.b128.conv0  426369      16400    [32, 128, 128, 128]  float16 \n","synthesis.b128.conv1  213249      16400    [32, 128, 128, 128]  float16 \n","synthesis.b128.torgb  66051       -        [32, 3, 128, 128]    float16 \n","synthesis.b128:0      -           16       [32, 128, 128, 128]  float16 \n","synthesis.b128:1      -           -        [32, 128, 128, 128]  float32 \n","---                   ---         ---      ---                  ---     \n","Total                 22949277    44448    -                    -       \n","\n","\n","Discriminator  Parameters  Buffers  Output shape         Datatype\n","---            ---         ---      ---                  ---     \n","b128.fromrgb   512         16       [32, 128, 128, 128]  float16 \n","b128.skip      32768       16       [32, 256, 64, 64]    float16 \n","b128.conv0     147584      16       [32, 128, 128, 128]  float16 \n","b128.conv1     295168      16       [32, 256, 64, 64]    float16 \n","b128           -           16       [32, 256, 64, 64]    float16 \n","b64.skip       131072      16       [32, 512, 32, 32]    float16 \n","b64.conv0      590080      16       [32, 256, 64, 64]    float16 \n","b64.conv1      1180160     16       [32, 512, 32, 32]    float16 \n","b64            -           16       [32, 512, 32, 32]    float16 \n","b32.skip       262144      16       [32, 512, 16, 16]    float16 \n","b32.conv0      2359808     16       [32, 512, 32, 32]    float16 \n","b32.conv1      2359808     16       [32, 512, 16, 16]    float16 \n","b32            -           16       [32, 512, 16, 16]    float16 \n","b16.skip       262144      16       [32, 512, 8, 8]      float16 \n","b16.conv0      2359808     16       [32, 512, 16, 16]    float16 \n","b16.conv1      2359808     16       [32, 512, 8, 8]      float16 \n","b16            -           16       [32, 512, 8, 8]      float16 \n","b8.skip        262144      16       [32, 512, 4, 4]      float32 \n","b8.conv0       2359808     16       [32, 512, 8, 8]      float32 \n","b8.conv1       2359808     16       [32, 512, 4, 4]      float32 \n","b8             -           16       [32, 512, 4, 4]      float32 \n","b4.mbstd       -           -        [32, 513, 4, 4]      float32 \n","b4.conv        2364416     16       [32, 512, 4, 4]      float32 \n","b4.fc          4194816     -        [32, 512]            float32 \n","b4.out         513         -        [32, 1]              float32 \n","---            ---         ---      ---                  ---     \n","Total          23882369    352      -                    -       \n","\n","Setting up augmentation...\n","Distributing across 1 GPUs...\n","Setting up training phases...\n","Exporting sample images...\n","Initializing logs...\n","Training for 10 kimg...\n","\n","tick 0     kimg 0.0      time 35s          sec/tick 8.2     sec/kimg 255.28  maintenance 26.6   cpumem 5.19   gpumem 10.90  augment 0.000\n","Evaluating metrics...\n","{\"results\": {\"fid50k_full\": 357.3046719543243}, \"metric\": \"fid50k_full\", \"total_time\": 535.5033547878265, \"total_time_str\": \"8m 56s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000000.pkl\", \"timestamp\": 1673601674.531423}\n","tick 1     kimg 4.0      time 12m 50s      sec/tick 188.4   sec/kimg 47.09   maintenance 547.3  cpumem 5.80   gpumem 7.18   augment 0.004\n","tick 2     kimg 8.0      time 15m 58s      sec/tick 187.5   sec/kimg 46.88   maintenance 0.2    cpumem 5.80   gpumem 7.18   augment 0.010\n","tick 3     kimg 10.0     time 17m 32s      sec/tick 93.2    sec/kimg 46.96   maintenance 0.2    cpumem 5.80   gpumem 7.17   augment 0.013\n","Evaluating metrics...\n","{\"results\": {\"fid50k_full\": 381.04603107160096}, \"metric\": \"fid50k_full\", \"total_time\": 531.9662165641785, \"total_time_str\": \"8m 52s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000010.pkl\", \"timestamp\": 1673602690.4734645}\n","\n","Exiting...\n"]}],"source":["!python train.py --outdir=/content/training-runs --data=/content/dataset.zip --kimg=10"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7091,"status":"ok","timestamp":1673603099716,"user":{"displayName":"Raj Kapadia","userId":"02452332357525858055"},"user_tz":-330},"id":"JHKXvlcRerKz","outputId":"e8ad1b78-86dc-4524-ffbd-73a1244138bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading networks from \"/content/training-runs/00001-dataset-auto1-kimg10/network-snapshot-000010.pkl\"...\n","Generating image for seed 85 (0/4) ...\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n","Generating image for seed 265 (1/4) ...\n","Generating image for seed 297 (2/4) ...\n","Generating image for seed 849 (3/4) ...\n"]}],"source":["!python generate.py --outdir=/content/out --trunc=1 --seeds=85,265,297,849 \\\n","    --network=/content/training-runs/00001-dataset-auto1-kimg10/network-snapshot-000010.pkl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgm0nYA2XCzF"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.1"}},"nbformat":4,"nbformat_minor":0}
